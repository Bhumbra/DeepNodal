"""Core Keras layers.
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import copy
import sys
import types as python_types
import warnings
import numbers
import numpy as np
import tensorflow as tf
from tensorflow.python.framework import tensor_util
from tensorflow.python.ops.nn_ops import _get_noise_shape
from tensorflow.python.eager import context
from tensorflow.python.framework import common_shapes
from tensorflow.python.framework import dtypes
from tensorflow.python.framework import ops
from tensorflow.python.framework import tensor_shape
from tensorflow.python.keras import activations
from tensorflow.python.keras import backend as K
from tensorflow.python.keras import constraints
from tensorflow.python.keras import initializers
from tensorflow.python.keras import regularizers
from tensorflow.python.keras.engine.base_layer import Layer
from tensorflow.python.keras.engine.input_spec import InputSpec
from tensorflow.python.keras.utils import conv_utils
from tensorflow.python.keras.utils import generic_utils
from tensorflow.python.keras.utils import tf_utils
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import gen_math_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import random_ops
from tensorflow.python.ops import state_ops
from tensorflow.python.ops import nn
from tensorflow.python.ops import nn_ops
from tensorflow.python.ops import standard_ops
from tensorflow.python.ops import variable_scope
from tensorflow.python.ops import variables as tf_variables
from tensorflow.python.util import nest
from tensorflow.python.util import tf_inspect
from tensorflow.python.util.tf_export import keras_export

ONE_MINUS_EXP_MINUS_ONE = 1. - float(np.exp(-1.))


#-------------------------------------------------------------------------------
def tf_dropout(x, rate, noise_shape=None, seed=None, name=None):
  """Computes dropout.

  With probability `rate`, drops elements of `x`. Input that are kept are
  scaled up by `1 / (1 - rate)`, otherwise outputs `0`.  The scaling is so that
  the expected sum is unchanged.

  **Note:** The behavior of dropout has changed between TensorFlow 1.x and 2.x.
  When converting 1.x code, please use named arguments to ensure behavior stays
  consistent.

  By default, each element is kept or dropped independently.  If `noise_shape`
  is specified, it must be
  [broadcastable](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)
  to the shape of `x`, and only dimensions with `noise_shape[i] == shape(x)[i]`
  will make independent decisions.  For example, if `shape(x) = [k, l, m, n]`
  and `noise_shape = [k, 1, 1, n]`, each batch and channel component will be
  kept independently and each row and column will be kept or not kept together.

  Args:
    x: A floating point tensor.
    rate: A scalar `Tensor` with the same type as x. The probability
      that each element is dropped. For example, setting rate=0.1 would drop
      10% of input elements.
    noise_shape: A 1-D `Tensor` of type `int32`, representing the
      shape for randomly generated keep/drop flags.
    seed: A Python integer. Used to create random seeds. See
      `tf.compat.v1.set_random_seed` for behavior.
    name: A name for this operation (optional).

  Returns:
    A Tensor of the same shape of `x`.

  Raises:
    ValueError: If `rate` is not in `(0, 1]` or if `x` is not a floating point
      tensor.
  """
  with ops.name_scope(name, "dropout", [x]) as name:
    x = ops.convert_to_tensor(x, name="x")
    if not x.dtype.is_floating:
      raise ValueError("x has to be a floating point tensor since it's going to"
                       " be scaled. Got a %s tensor instead." % x.dtype)
    if isinstance(rate, numbers.Real):
      if not (rate >= 0 and rate < 1):
        raise ValueError("rate must be a scalar tensor or a float in the "
                         "range [0, 1), got %g" % rate)
      if rate > 0.5:
        logging.log_first_n(
            logging.WARN, "Large dropout rate: %g (>0.5). In TensorFlow "
            "2.x, dropout() uses dropout rate instead of keep_prob. "
            "Please ensure that this is intended.", 5, rate)

    # Early return if nothing needs to be dropped.
    if isinstance(rate, numbers.Real) and rate == 0:
      return x
    rate = ops.convert_to_tensor(
        rate, dtype=x.dtype, name="rate")
    rate.get_shape().assert_is_compatible_with(tensor_shape.scalar())

    # Do nothing if we know rate == 0
    if tensor_util.constant_value(rate) == 1:
      return x

    noise_shape = _get_noise_shape(x, noise_shape)
    # Sample a uniform distribution on [0.0, 1.0) and select values larger than
    # rate.
    #
    # NOTE: Random uniform actually can only generate 2^23 floats on [1.0, 2.0)
    # and subtract 1.0.
    random_tensor = random_ops.random_uniform(
        noise_shape, seed=seed, dtype=x.dtype)
    keep_prob = 1 - rate
    scale = 1 / keep_prob
    # NOTE: if (1.0 + rate) - 1 is equal to rate, then we want to consider that
    # float to be selected, hence we use a >= comparison.
    keep_mask = random_tensor >= rate
    ret = x * scale * math_ops.cast(keep_mask, x.dtype)
    ret.set_shape(x.get_shape())
    return ret


#-------------------------------------------------------------------------------
def betarnd(shape, alpha, beta):
  x = random_ops.random_gamma(shape, alpha, 1.)
  y = random_ops.random_gamma(shape, beta, 1.)
  return x / (x+y)

#-------------------------------------------------------------------------------
def qsd(x, rate=None, prec=None, 
           dist_drop=False, dist_coef=False, dist_couple=False, mask_axes=None, 
           noise_shape=None, seed=None, name=None):
  if mask_axes:
    if type(mask_axes) is int:
      mask_axes = [mask_axes]
  with ops.name_scope(name, "dropout", [x]) as name:
    x = ops.convert_to_tensor(x, name="x")
    if not x.dtype.is_floating:
      raise ValueError("x has to be a floating point tensor since it's going to"
                       " be scaled. Got a %s tensor instead." % x.dtype)
    if isinstance(rate, numbers.Real):
      if not (rate >= 0 and rate < 1):
        raise ValueError("rate must be a scalar tensor or a float in the "
                         "range [0, 1), got %g" % rate)
      if rate > 0.5:
        logging.log_first_n(
            logging.WARN, "Large dropout rate: %g (>0.5). In TensorFlow "
            "2.x, dropout() uses dropout rate instead of keep_prob. "
            "Please ensure that this is intended.", 5, rate)

    # Early return if nothing needs to be dropped.
    if isinstance(rate, numbers.Real) and rate == 0:
      return x
    rate = ops.convert_to_tensor(
        rate, dtype=x.dtype, name="rate")
    rate.get_shape().assert_is_compatible_with(tensor_shape.scalar())
    # Do nothing if we know rate == 0
    if tensor_util.constant_value(rate) == 0:
      return x
    keep_prob = 1 - rate

    # Evaluated noise and broadcasting dimensions
    noise_shape = _get_noise_shape(x, noise_shape)
    noise_length = int(len(noise_shape))
    cdf_dims = noise_shape
    thr_dims = [1] * noise_length
    rep_dims = [1] * noise_length
    if mask_axes is None:
      for i in range(1, noise_length):
        thr_dims[i] = noise_shape[i]
    else:
      cdf_dims = [1] * noise_length
      for i in range(noise_length):
        if i == 0 or i in mask_axes:
          cdf_dims[i] = noise_shape[i]
          if i in mask_axes:
            thr_dims[i] = noise_shape[i]
        else:
          rep_dims[i] = noise_shape[i]
    
    # Evaluate thresholds and scale coefficients 
    thresholds = None
    scale_coef = None
    if prec is not None:
      alpha = ops.convert_to_tensor(prec, dtype=x.dtype, name="alpha")
      alpha.get_shape().assert_is_compatible_with(tensor_shape.scalar())
      beta = alpha * keep_prob / rate
      alpha_dist = tf.random.gamma(thr_dims, gamma, 1.)
      alpha_dist = tf.random.gamma(thr_dims, gamma, 1.)
      alpha = gamma * keep_prob / rate
      alpha_dist = tf.random.gamma(thr_dims, alpha, inverse_lmbda)
      beta_dist = alpha_dist / (alpha_dist + gamma_dist)
      if dist_drop:
        thresholds = array_ops.reshape(beta_dist, thr_dims)
      if dist_coef:
        scale_coef = gamma_dist
        if dist_drop and dist_couple: # correction for cross-dependencies
          scale_coef = gamma_dist * (gamma + rate) / gamma
        elif not dist_couple:
          scale_coef = tf.reshape(tf.random.shuffle(tf.reshape(gamma_dist, [-1])), thr_dims)
      else:
        scale_coef = 1 / keep_prob
    elif mask_axes is not None:
      thresholds = array_ops.reshape(keep_prob, [1] * noise_length)
      scale_coef = array_ops.reshape(1 / keep_prob, [1] * noise_length)

    if thresholds is None:
      thresholds = keep_prob
    if scale_coef is None:
      scale_coef = 1 / keep_prob

    # Sample a uniform distribution on [0.0, 1.0) and select values >= rate
    random_cdf = random_ops.random_uniform(cdf_dims, seed=seed, dtype=x.dtype)
    keep_mask = math_ops.cast(random_cdf < thresholds, x.dtype)
    coef_mask = keep_mask * scale_coef
    y = x * coef_mask
    y.set_shape(x.get_shape())
    return y

#-------------------------------------------------------------------------------
class QSD(Layer):
  _inputs_shape = None
  _reduce_axes = None

  def __init__(self, rate, prec=None, dist_drop=False, dist_coef=False, dist_couple=False, 
               prec_step=None, mask_axes=None, noise_shape=None, seed=None, **kwargs):
    super(QSD, self).__init__(**kwargs)
    self.rate = rate
    self.prec = prec
    self.dist_drop = dist_drop
    self.dist_coef = dist_coef
    self.dist_couple = dist_couple
    self.prec_step = prec_step
    self.mask_axes = mask_axes
    self.noise_shape = noise_shape
    self.seed = seed
    self.supports_masking = True
    if type(self.mask_axes) is int:
      self.mask_axes = [self.mask_axes]

  def _get_noise_shape(self, inputs):
    # Subclasses of `Dropout` may implement `_get_noise_shape(self, inputs)`,
    # which will override `self.noise_shape`, and allows for custom noise
    # shapes with dynamically sized inputs.
    if self.noise_shape is None:
      return self.noise_shape
    return nn_ops._get_noise_shape(inputs, self.noise_shape)  # pylint: disable=protected-access

  def call(self, inputs, training=None):
    self._inputs_shape = inputs.shape
    noise_shape = self._get_noise_shape(inputs)
    self._reduce_axes = [0] 
    state_shape = self._inputs_shape[1:]
    if self.mask_axes is not None:
      state_shape = []
      for i in range(1, len(self._inputs_shape)):
        if i in self.mask_axes:
          state_shape.append(self._inputs_shape[i])
        else:
          self._reduce_axes.append(i)
    if self.prec is not None:
      self.moving_t = self.add_weight(name='moving_t',
                                      shape=(),
                                      dtype=inputs.dtype,
                                      initializer='zeros',
                                      synchronization=tf_variables.VariableSynchronization.ON_READ,
                                      trainable=False,
                                      aggregation=tf_variables.VariableAggregation.MEAN,
                                      experimental_autocast=False)

    if training is None:
      training = K.learning_phase()

    def dropped_inputs():
      prec = None
      if self.prec is None and self.mask_axes is None:
        return tf_dropout(
            inputs,
            noise_shape=noise_shape,
            seed=self.seed,
            rate=self.rate)
      if self.prec:
        prec = self.prec if not self.prec_step \
           else self.prec + self.prec * self.prec_step * self.moving_t
      return qsd(
                  inputs,
                  rate=self.rate,
                  prec=prec,
                  dist_drop=self.dist_drop,
                  dist_coef=self.dist_coef,
                  dist_couple=self.dist_couple,
                  mask_axes=self.mask_axes,
                  noise_shape=noise_shape,
                  seed=self.seed,
                )

    def identity_inputs():
      return tf.identity(inputs)

    if self.prec is None and self.mask_axes is None:
      output = tf_utils.smart_cond(training,
                                   dropped_inputs,
                                   lambda: array_ops.identity(inputs))
    else:
      output = tf_utils.smart_cond(training,
                                   dropped_inputs,
                                   identity_inputs)

    if self.prec is not None:
      if self.prec_step is not None:
        def t_update():
          return self._assign_t(self.moving_t)
        self.add_update(t_update)

    return output

  def _assign_t(self, moving_t):
    with K.name_scope('AssignMovingT') as scope:
      return state_ops.assign(moving_t, math_ops.add(moving_t, 1))

  def compute_output_shape(self, input_shape):
    return input_shape

  def get_config(self):
    config = {
        'rate': self.rate,
        'noise_shape': self.noise_shape,
        'seed': self.seed
    }
    base_config = super(Dropout, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))

#-------------------------------------------------------------------------------
